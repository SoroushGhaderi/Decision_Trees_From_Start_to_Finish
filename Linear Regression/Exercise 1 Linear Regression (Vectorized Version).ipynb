{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with multiple variables\n",
    "\n",
    "In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n",
    "\n",
    "The file `Multi_linear.txt` contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data file and shape\n",
    "data = pd.read_csv('Multi_linear.txt', header = None)\n",
    "number_of_samples, number_of_features  = data.shape\n",
    "\n",
    "#Initializing X and Y according to shape and converting to numpy arrays\n",
    "X = data.iloc[:,0:number_of_features-1].values\n",
    "y = data.iloc[:,number_of_features-1:number_of_features].values\n",
    "\n",
    "#Initializing theta\n",
    "theta = np.zeros((number_of_features,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Equations\n",
    "\n",
    "The objective of linear regression is to minimize the cost function\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m}(X\\theta - y)^T(X\\theta - y)\\qquad\\text{(vectorized version)}$$\n",
    "\n",
    "Recall that the parameters of your model are the $\\theta_j$ values. These are\n",
    "the values you will adjust to minimize cost $J(\\theta)$. One way to do this is to\n",
    "use the batch gradient descent algorithm. In batch gradient descent, each\n",
    "iteration performs the update\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta} = \\frac{1}{m}X^T(X\\theta - y)   $$\n",
    "\n",
    "With each step of gradient descent, your parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost J($\\theta$).\\\n",
    "In previous notebook we discover linear regression with normal `cost_function` formula but in this section we implement `cost_function` and `gradient_descent` with vectorized version.\\\n",
    "The time complexity of vectorization method is O(s), where s in the number of iterations. As contrast, the for loop approach time complexity is O($s\\times n\\times m\\times n$), where s is the iterations, m is the dataset sample number, n is the dataset feature number. Therefore, using vectorization in our machine leaning algorithm is the key to boost your algorithm and save you a huge amount of training time. Vectorization would be a great approach we need to consider and worth to spent time on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization\n",
    "\n",
    "We start by loading and displaying some values from this dataset. By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly.\\\n",
    "Your task here is to complete the code in `feature_normalization` function:\n",
    "- Subtract the mean value of each feature from the dataset.\n",
    "- After subtracting the mean, additionally scale (divide) the feature values by their respective “standard deviations.”\n",
    "\n",
    "The standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (most data points will lie within ±2 standard deviations of the mean); this is an alternative to taking the range of values (max-min). In `numpy`, you can use the `std` function to compute the standard deviation. \n",
    "\n",
    "For example, the quantity `X[:, 0]` contains all the values of $x_1$ (house sizes) in the training set, so `np.std(X[:, 0])` computes the standard deviation of the house sizes.\n",
    "At the time that the function `feature_normalization` is called, the extra column of 1’s corresponding to $x_0 = 1$ has not yet been added to $X$. \n",
    "\n",
    "You will do this for all the features and your code should work with datasets of all sizes (any number of features / examples). Note that each column of the matrix $X$ corresponds to one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalization(X):\n",
    "    \"\"\"\n",
    "    Normalizes the features in X. returns a normalized version of X where\n",
    "    the mean value of each feature is 0 and the standard deviation\n",
    "    is 1. This is often a good preprocessing step to do when working with\n",
    "    learning algorithms.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_normalized : array_like\n",
    "        The normalized dataset of shape (m x n).\n",
    "    mean: array_like\n",
    "        Mean of every column\n",
    "    standard_deviation : array_like\n",
    "        Standard Deviation of every column\n",
    "    \"\"\"\n",
    "    standard_deviation = np.std(X, axis= 0)\n",
    "    mean = np.mean(X, axis= 0)\n",
    "    X_normalized = (X - mean)/ standard_deviation\n",
    "    \n",
    "    return X_normalized, mean, standard_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_X, mean_X, standard_deviation_X = feature_normalization(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "We have already set up the data for linear regression. In the following cell, we add another dimension to our data to accommodate the $\\theta_0$ intercept term. Do NOT execute this cell more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the columns of 1s to X \n",
    "X = np.concatenate((np.ones((number_of_samples,1)),normalized_X), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the cost $J(\\theta)$\n",
    "\n",
    "As you perform gradient descent to learn minimize the cost function $J(\\theta)$, it is helpful to monitor the convergence by computing the cost. In this section, you will implement a function to calculate $J(\\theta)$ so you can check the convergence of your gradient descent implementation. \n",
    "\n",
    "Your next task is to complete the code for the function `cost_function` which computes $J(\\theta)$. As you are doing this, remember that the variables $X$ and $y$ are not scalar values. $X$ is a matrix whose rows represent the examples from the training set and $y$ is a vector whose each elemennt represent the value at a given row of $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X,y,theta):\n",
    "    \"\"\"\n",
    "    Compute cost for linear regression with multiple variables.\n",
    "    Computes the cost of using theta as the parameter for linear regression to fit the data points in X and y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n+1).\n",
    "    \n",
    "    y : array_like\n",
    "        A vector of shape (m, ) for the values at a given data point.\n",
    "    \n",
    "    theta : array_like\n",
    "        The linear regression parameters. A vector of shape (n+1, )\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The value of the cost function. \n",
    "    \"\"\"\n",
    "    #Initialisation of useful values \n",
    "    m = np.size(y)\n",
    "    J = 0\n",
    "    \n",
    "    #Hypothesis function in vectorized form\n",
    "    h = np.dot(X,theta)\n",
    "\n",
    "    #Cost function in vectorized form\n",
    "    J = float((1./(2*m)) * np.dot((h - y).T, (h - y)));    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Gradient descent\n",
    "\n",
    "Next, you will complete a function which implements gradient descent.\\\n",
    "As you program, make sure you understand what you are trying to optimize and what is being updated. Keep in mind that the cost $J(\\theta)$ is parameterized by the vector $\\theta$, not $X$ and $y$. That is, we minimize the value of $J(\\theta)$ by changing the values of the vector $\\theta$, not by changing $X$ or $y$. A good way to verify that gradient descent is working correctly is to look at the value of $J(\\theta)$ and check that it is decreasing with each step. \n",
    "\n",
    "The starter code for the function `gradient_descent` calls `cost_function` on every iteration and saves the cost to a `python` list. Assuming you have implemented gradient descent and `cost_function` correctly, your value of $J(\\theta)$ should never increase, and should converge to a steady value by the end of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, learning_rate = 0.0005, iteration=1000):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn theta.\n",
    "    Updates theta by taking num_iters gradient steps with learning rate alpha.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n+1).\n",
    "    \n",
    "    y : array_like\n",
    "        A vector of shape (m, ) for the values at a given data point.\n",
    "    \n",
    "    theta : array_like\n",
    "        The linear regression parameters. A vector of shape (n+1, )\n",
    "    \n",
    "    learning_rate : float\n",
    "        The learning rate for gradient descent. \n",
    "    \n",
    "    iteration : int\n",
    "        The number of iterations to run gradient descent. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta : array_like\n",
    "        The learned linear regression parameters. A vector of shape (n+1, ).\n",
    "    \n",
    "    J_history : list\n",
    "        A python list for the values of the cost function after each iteration.\n",
    "        \n",
    "    theta_history : list\n",
    "        A python list for the values of theta per each iteration and store them in a list\n",
    "    \n",
    "    \"\"\"\n",
    "    #Initialisation of useful values \n",
    "    m = np.size(y)\n",
    "    J_history = np.zeros(iteration)\n",
    "\n",
    "    for i in range(iteration):\n",
    "        #Hypothesis function\n",
    "        h = np.dot(X,theta)\n",
    "        \n",
    "        #Calculating the grad function in vectorized form\n",
    "        theta = theta - learning_rate * (1/m)* (X.T.dot(h-y))\n",
    "        J_history[i] = cost_function(X,y,theta)\n",
    "    optimized_hypothesis= h\n",
    "    return theta, J_history, optimized_hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameter - theta is defined before\n",
    "learning_rate= .02\n",
    "iteration= 1000\n",
    "# run gradient descent\n",
    "theta_optimized , cost_function_history, optimized_hypothesis= gradient_descent(X, y, theta, learning_rate, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[340412.65900156],\n",
       "       [109439.39158281],\n",
       "       [ -6569.94996733]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize= (12,7))\n",
    "plt.plot(range(1000), cost_function_history, color= \"teal\", linewidth= 3)\n",
    "plt.xlabel(\"# Iteration\")\n",
    "plt.ylabel(\"Cost Value\")\n",
    "plt.title(\"Cost value during Iteration\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
